{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Unsupervised Arabic Sentiment Analysis\n",
        "\n",
        "This notebook analyzes Arabic text sentiment **without relying on potentially incorrect labels**.\n",
        "\n",
        "## Approaches:\n",
        "1. **Lexicon-based**: Using Arabic sentiment word dictionaries\n",
        "2. **Rule-based**: Pattern matching for positive/negative expressions\n",
        "3. **Clustering**: Grouping similar texts to discover sentiment patterns\n",
        "4. **Weak supervision**: Creating rules from text patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "## 1. Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_libs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script tabulate.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script isympy.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script f2py.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script muddler.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts futurize.exe and pasteurize.exe are installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script nltk.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts camel_arclean.exe, camel_data.exe, camel_dediac.exe, camel_diac.exe, camel_morphology.exe, camel_transliterate.exe and camel_word_tokenize.exe are installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q kagglehub nltk scikit-learn pandas numpy camel-tools ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "import_cell"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_header"
      },
      "source": [
        "## 2. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded to: C:\\Users\\pc\\.cache\\kagglehub\\datasets\\saurabhshahane\\arabic-classification\\versions\\1\n",
            "\n",
            "Dataset shape: (111728, 2)\n",
            "Columns: ['text', 'targe']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>targe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>تزال صناعة الجلود في المغرب تتبع الطريقة التقل...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  targe\n",
              "0  بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...      0\n",
              "1  قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...      0\n",
              "2  أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...      0\n",
              "3  اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...      0\n",
              "4  تزال صناعة الجلود في المغرب تتبع الطريقة التقل...      0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download Arabic classification dataset\n",
        "path = kagglehub.dataset_download(\"saurabhshahane/arabic-classification\")\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = os.path.join(path, 'arabic_dataset_classifiction.csv')\n",
        "if os.path.isdir(csv_file_path):\n",
        "    files = [f for f in os.listdir(csv_file_path) if f.endswith('.csv')]\n",
        "    if files:\n",
        "        csv_file_path = os.path.join(csv_file_path, files[0])\n",
        "\n",
        "df = pd.read_csv(csv_file_path)\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess_header"
      },
      "source": [
        "## 3. Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "preprocess_cell"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing text...\n",
            "✓ Processed 108787 texts\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...</td>\n",
              "      <td>استوديوهات ورزازات وصحراء مرزوكه واثار وليلي ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...</td>\n",
              "      <td>قررت النجمه الامريكيه اوبرا وينفري الا يقتصر ع...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...</td>\n",
              "      <td>اخبارنا المغربيه الوزاني تصوير الشملالي الهب ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...</td>\n",
              "      <td>اخبارنا المغربيه قال ابراهيم الراشدي محامي سعد...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>تزال صناعة الجلود في المغرب تتبع الطريقة التقل...</td>\n",
              "      <td>تزال صناعه الجلود المغرب تتبع الطريقه التقليدي...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...   \n",
              "1  قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...   \n",
              "2  أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...   \n",
              "3  اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...   \n",
              "4  تزال صناعة الجلود في المغرب تتبع الطريقة التقل...   \n",
              "\n",
              "                                        cleaned_text  \n",
              "0  استوديوهات ورزازات وصحراء مرزوكه واثار وليلي ا...  \n",
              "1  قررت النجمه الامريكيه اوبرا وينفري الا يقتصر ع...  \n",
              "2  اخبارنا المغربيه الوزاني تصوير الشملالي الهب ا...  \n",
              "3  اخبارنا المغربيه قال ابراهيم الراشدي محامي سعد...  \n",
              "4  تزال صناعه الجلود المغرب تتبع الطريقه التقليدي...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "arabic_stopwords = set(stopwords.words('arabic'))\n",
        "\n",
        "def preprocess_arabic_text(text, remove_stopwords=True):\n",
        "    \"\"\"Clean and normalize Arabic text\"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "    \n",
        "    # Remove diacritics\n",
        "    text = re.sub(r'[\\u064B-\\u0652\\u0653\\u0670]', '', text)\n",
        "    \n",
        "    # Remove Tatweel\n",
        "    text = re.sub(r'\\u0640', '', text)\n",
        "    \n",
        "    # Normalize characters\n",
        "    text = re.sub(r'[\\u0622\\u0623\\u0625]', '\\u0627', text)  # Alif forms\n",
        "    text = re.sub(r'\\u0649', '\\u064a', text)  # Alef Maksura to Yeh\n",
        "    text = re.sub(r'\\u0629', '\\u0647', text)  # Ta Marbuta to Ha\n",
        "    \n",
        "    # Keep only Arabic letters and spaces\n",
        "    text = re.sub(r'[^\\u0621-\\u064A\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Tokenize and optionally remove stopwords\n",
        "    tokens = text.split()\n",
        "    if remove_stopwords:\n",
        "        tokens = [w for w in tokens if w not in arabic_stopwords and len(w) > 1]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Preprocessing text...\")\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_arabic_text)\n",
        "df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "print(f\"✓ Processed {len(df)} texts\")\n",
        "df[['text', 'cleaned_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lexicon_header"
      },
      "source": [
        "## 4. Build Arabic Sentiment Lexicon\n",
        "\n",
        "We'll create sentiment lexicons by analyzing word co-occurrence patterns in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "seed_words"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive seed words: 30\n",
            "Negative seed words: 31\n",
            "Intensifiers: 7\n",
            "Negations: 7\n"
          ]
        }
      ],
      "source": [
        "# Common Arabic positive and negative seed words\n",
        "# These are starting points that we'll expand\n",
        "\n",
        "POSITIVE_SEEDS = {\n",
        "    'رائع', 'ممتاز', 'جميل', 'عظيم', 'مذهل', 'سعيد', 'حب', 'احب',\n",
        "    'افضل', 'جيد', 'مفيد', 'ناجح', 'نجاح', 'سرور', 'بهجه', 'فرح',\n",
        "    'ممل', 'مشوق', 'رائه', 'حلو', 'لطيف', 'جذاب', 'مثالي',\n",
        "    'احسن', 'اجمل', 'اروع', 'يستحق', 'مميز', 'فائق', 'متميز'\n",
        "}\n",
        "\n",
        "NEGATIVE_SEEDS = {\n",
        "    'سيء', 'سيي', 'فشل', 'فاشل', 'سيئه', 'غضب', 'حزن', 'حزين',\n",
        "    'كره', 'اكره', 'مشكله', 'خطا', 'خطأ', 'ضعيف', 'مروع', 'فظيع',\n",
        "    'رديء', 'ردي', 'اسوء', 'بشع', 'مقرف', 'مزعج', 'محبط',\n",
        "    'سلبي', 'كارثه', 'كريه', 'مريع', 'مخيب', 'غير', 'لم', 'ليس'\n",
        "}\n",
        "\n",
        "INTENSIFIERS = {\n",
        "    'جدا', 'للغايه', 'كثيرا', 'حقا', 'فعلا', 'تماما', 'ابدا'\n",
        "}\n",
        "\n",
        "NEGATIONS = {\n",
        "    'لا', 'ليس', 'لم', 'لن', 'غير', 'بدون', 'ما'\n",
        "}\n",
        "\n",
        "print(f\"Positive seed words: {len(POSITIVE_SEEDS)}\")\n",
        "print(f\"Negative seed words: {len(NEGATIVE_SEEDS)}\")\n",
        "print(f\"Intensifiers: {len(INTENSIFIERS)}\")\n",
        "print(f\"Negations: {len(NEGATIONS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lexicon_analysis"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing lexicon-based sentiment analysis:\n",
            "\n",
            "Text: هذا المنتج رائع جدا\n",
            "  → Positive (score: +2, confidence: 0.40)\n",
            "    ✅ Positive: رائع جدا\n",
            "\n",
            "Text: الخدمه سيئه للغايه\n",
            "  → Negative (score: -2, confidence: 0.40)\n",
            "    ❌ Negative: سيئه للغايه\n",
            "\n",
            "Text: ليس سيئا\n",
            "  → Somewhat Negative (score: -1, confidence: 0.60)\n",
            "    ❌ Negative: ليس\n",
            "\n",
            "Text: جيد جدا واحبه\n",
            "  → Positive (score: +2, confidence: 0.40)\n",
            "    ✅ Positive: جيد جدا\n",
            "\n",
            "Text: لا احب هذا المنتج\n",
            "  → Somewhat Negative (score: -1, confidence: 0.60)\n",
            "    ❌ Negative: لا احب\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def analyze_text_sentiment_lexicon(text, positive_words, negative_words, \n",
        "                                   intensifiers, negations):\n",
        "    \"\"\"\n",
        "    Analyze sentiment using lexicon-based approach.\n",
        "    Returns sentiment score and explanation.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    \n",
        "    positive_score = 0\n",
        "    negative_score = 0\n",
        "    \n",
        "    positive_matches = []\n",
        "    negative_matches = []\n",
        "    \n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "        \n",
        "        # Check for negation + sentiment word patterns\n",
        "        if i < len(tokens) - 1:\n",
        "            next_token = tokens[i + 1]\n",
        "            \n",
        "            # Negation flips sentiment\n",
        "            if token in negations:\n",
        "                if next_token in positive_words:\n",
        "                    negative_score += 1\n",
        "                    negative_matches.append(f\"{token} {next_token}\")\n",
        "                    i += 2\n",
        "                    continue\n",
        "                elif next_token in negative_words:\n",
        "                    positive_score += 0.5  # Double negative\n",
        "                    positive_matches.append(f\"{token} {next_token}\")\n",
        "                    i += 2\n",
        "                    continue\n",
        "            \n",
        "            # Intensifier amplifies sentiment\n",
        "            if next_token in intensifiers:\n",
        "                if token in positive_words:\n",
        "                    positive_score += 2\n",
        "                    positive_matches.append(f\"{token} {next_token}\")\n",
        "                    i += 2\n",
        "                    continue\n",
        "                elif token in negative_words:\n",
        "                    negative_score += 2\n",
        "                    negative_matches.append(f\"{token} {next_token}\")\n",
        "                    i += 2\n",
        "                    continue\n",
        "        \n",
        "        # Single word sentiment\n",
        "        if token in positive_words:\n",
        "            positive_score += 1\n",
        "            positive_matches.append(token)\n",
        "        elif token in negative_words:\n",
        "            negative_score += 1\n",
        "            negative_matches.append(token)\n",
        "        \n",
        "        i += 1\n",
        "    \n",
        "    # Calculate final score\n",
        "    net_score = positive_score - negative_score\n",
        "    \n",
        "    # Determine sentiment category\n",
        "    if net_score >= 2:\n",
        "        sentiment = 'Positive'\n",
        "        confidence = min(abs(net_score) / 5, 1.0)\n",
        "    elif net_score >= 1:\n",
        "        sentiment = 'Somewhat Positive'\n",
        "        confidence = 0.6\n",
        "    elif net_score <= -2:\n",
        "        sentiment = 'Negative'\n",
        "        confidence = min(abs(net_score) / 5, 1.0)\n",
        "    elif net_score <= -1:\n",
        "        sentiment = 'Somewhat Negative'\n",
        "        confidence = 0.6\n",
        "    else:\n",
        "        sentiment = 'Neutral'\n",
        "        confidence = 0.5\n",
        "    \n",
        "    return {\n",
        "        'sentiment': sentiment,\n",
        "        'score': net_score,\n",
        "        'positive_score': positive_score,\n",
        "        'negative_score': negative_score,\n",
        "        'confidence': confidence,\n",
        "        'positive_matches': positive_matches,\n",
        "        'negative_matches': negative_matches\n",
        "    }\n",
        "\n",
        "# Test on sample texts\n",
        "samples = [\n",
        "    \"هذا المنتج رائع جدا\",\n",
        "    \"الخدمه سيئه للغايه\",\n",
        "    \"ليس سيئا\",\n",
        "    \"جيد جدا واحبه\",\n",
        "    \"لا احب هذا المنتج\"\n",
        "]\n",
        "\n",
        "print(\"Testing lexicon-based sentiment analysis:\\n\")\n",
        "for sample in samples:\n",
        "    cleaned = preprocess_arabic_text(sample, remove_stopwords=False)\n",
        "    result = analyze_text_sentiment_lexicon(\n",
        "        cleaned, POSITIVE_SEEDS, NEGATIVE_SEEDS, \n",
        "        INTENSIFIERS, NEGATIONS\n",
        "    )\n",
        "    print(f\"Text: {sample}\")\n",
        "    print(f\"  → {result['sentiment']} (score: {result['score']:+d}, confidence: {result['confidence']:.2f})\")\n",
        "    if result['positive_matches']:\n",
        "        print(f\"    ✅ Positive: {', '.join(result['positive_matches'])}\")\n",
        "    if result['negative_matches']:\n",
        "        print(f\"    ❌ Negative: {', '.join(result['negative_matches'])}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apply_lexicon_header"
      },
      "source": [
        "## 5. Apply Lexicon-Based Analysis to Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "apply_lexicon"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing sentiment for all texts...\n",
            "✓ Analysis complete\n",
            "\n",
            "Sentiment Distribution (Lexicon-based):\n",
            "lexicon_sentiment\n",
            "Neutral              81575\n",
            "Somewhat Positive    10651\n",
            "Somewhat Negative     8320\n",
            "Positive              5081\n",
            "Negative              3160\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Average confidence by sentiment:\n",
            "lexicon_sentiment\n",
            "Negative             0.52\n",
            "Neutral              0.50\n",
            "Positive             0.54\n",
            "Somewhat Negative    0.60\n",
            "Somewhat Positive    0.60\n",
            "Name: lexicon_confidence, dtype: float64\n",
            "\n",
            "Sample results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lexicon_sentiment</th>\n",
              "      <th>lexicon_score</th>\n",
              "      <th>positive_matches</th>\n",
              "      <th>negative_matches</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...</td>\n",
              "      <td>Somewhat Positive</td>\n",
              "      <td>1.0</td>\n",
              "      <td>سعيد</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>تزال صناعة الجلود في المغرب تتبع الطريقة التقل...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>مصطفى الصوفي انطلقت أمس فعاليات الدورة الثالثة...</td>\n",
              "      <td>Somewhat Positive</td>\n",
              "      <td>1.0</td>\n",
              "      <td>احسن</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>الاتحاد الاشتراكي تحيي الفنانة اللبنانية نجوى ...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>أقدمت كاثريونا وايت حبيبة الممثل جيم كاري على ...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>أحمد الريدي تطور جديد شهدته قضية الفنانة زينة ...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>أخبارنا المغربية بشراكة بين مؤسسة البشير للتعل...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  lexicon_sentiment  \\\n",
              "0  بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...            Neutral   \n",
              "1  قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...            Neutral   \n",
              "2  أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...  Somewhat Positive   \n",
              "3  اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...            Neutral   \n",
              "4  تزال صناعة الجلود في المغرب تتبع الطريقة التقل...            Neutral   \n",
              "5  مصطفى الصوفي انطلقت أمس فعاليات الدورة الثالثة...  Somewhat Positive   \n",
              "6  الاتحاد الاشتراكي تحيي الفنانة اللبنانية نجوى ...            Neutral   \n",
              "7  أقدمت كاثريونا وايت حبيبة الممثل جيم كاري على ...            Neutral   \n",
              "8  أحمد الريدي تطور جديد شهدته قضية الفنانة زينة ...            Neutral   \n",
              "9  أخبارنا المغربية بشراكة بين مؤسسة البشير للتعل...            Neutral   \n",
              "\n",
              "   lexicon_score positive_matches negative_matches  \n",
              "0            0.0                                    \n",
              "1            0.0                                    \n",
              "2            1.0             سعيد                   \n",
              "3            0.0                                    \n",
              "4            0.0                                    \n",
              "5            1.0             احسن                   \n",
              "6            0.0                                    \n",
              "7            0.0                                    \n",
              "8            0.0                                    \n",
              "9            0.0                                    "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Analyze all texts\n",
        "print(\"Analyzing sentiment for all texts...\")\n",
        "\n",
        "results = []\n",
        "for text in df['cleaned_text']:\n",
        "    result = analyze_text_sentiment_lexicon(\n",
        "        text, POSITIVE_SEEDS, NEGATIVE_SEEDS,\n",
        "        INTENSIFIERS, NEGATIONS\n",
        "    )\n",
        "    results.append(result)\n",
        "\n",
        "# Add results to dataframe\n",
        "df['lexicon_sentiment'] = [r['sentiment'] for r in results]\n",
        "df['lexicon_score'] = [r['score'] for r in results]\n",
        "df['lexicon_confidence'] = [r['confidence'] for r in results]\n",
        "df['positive_matches'] = [', '.join(r['positive_matches']) for r in results]\n",
        "df['negative_matches'] = [', '.join(r['negative_matches']) for r in results]\n",
        "\n",
        "print(\"✓ Analysis complete\\n\")\n",
        "\n",
        "# Show distribution\n",
        "print(\"Sentiment Distribution (Lexicon-based):\")\n",
        "print(df['lexicon_sentiment'].value_counts())\n",
        "\n",
        "print(\"\\nAverage confidence by sentiment:\")\n",
        "print(df.groupby('lexicon_sentiment')['lexicon_confidence'].mean().round(2))\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nSample results:\")\n",
        "df[['text', 'lexicon_sentiment', 'lexicon_score', 'positive_matches', 'negative_matches']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "discover_header"
      },
      "source": [
        "## 6. Discover New Sentiment Words\n",
        "\n",
        "Use co-occurrence analysis to find words that frequently appear with positive/negative seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "discover_words"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovering new sentiment words from the corpus...\n",
            "\n",
            "Discovered Positive Words:\n",
            "Word\t\tCount\tRatio\n",
            "----------------------------------------\n",
            "العربي         \t3368\t0.67\n",
            "العربيه        \t3285\t0.67\n",
            "الفنان         \t2902\t0.83\n",
            "مجال           \t2848\t0.71\n",
            "البرنامج       \t2541\t0.69\n",
            "الفنيه         \t2490\t0.79\n",
            "ملال           \t2418\t0.73\n",
            "الفيلم         \t2405\t0.80\n",
            "عمل            \t2309\t0.67\n",
            "فيلم           \t2254\t0.83\n",
            "الفني          \t2245\t0.73\n",
            "بني            \t2128\t0.71\n",
            "المهرجان       \t1991\t0.86\n",
            "اغنيه          \t1963\t0.86\n",
            "جائزه          \t1875\t0.85\n",
            "\n",
            "Discovered Negative Words:\n",
            "Word\t\tCount\tRatio\n",
            "----------------------------------------\n",
            "القدم          \t18528\t0.67\n",
            "جامعه          \t3358\t0.76\n",
            "لقجع           \t2039\t0.70\n",
            "والرياضه       \t1556\t0.67\n",
            "السله          \t1236\t0.75\n",
            "المديري        \t1011\t0.74\n",
            "المتقي         \t934\t0.67\n",
            "الفهري         \t692\t0.75\n",
            "الفيفا         \t643\t0.69\n",
            "بالجامعه       \t570\t0.68\n",
            "فرع            \t566\t0.67\n",
            "النسويه        \t559\t0.70\n",
            "اوزين          \t550\t0.68\n",
            "الشغب          \t526\t0.70\n",
            "بلاتر          \t472\t0.73\n"
          ]
        }
      ],
      "source": [
        "def discover_sentiment_words(texts, seed_positive, seed_negative, top_n=30):\n",
        "    \"\"\"\n",
        "    Discover new sentiment words based on co-occurrence with seed words.\n",
        "    \"\"\"\n",
        "    positive_cooccur = Counter()\n",
        "    negative_cooccur = Counter()\n",
        "    \n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        \n",
        "        # Check if text contains seed words\n",
        "        has_positive = any(token in seed_positive for token in tokens)\n",
        "        has_negative = any(token in seed_negative for token in tokens)\n",
        "        \n",
        "        # Count co-occurring words\n",
        "        if has_positive:\n",
        "            for token in tokens:\n",
        "                if token not in seed_positive and token not in seed_negative:\n",
        "                    positive_cooccur[token] += 1\n",
        "        \n",
        "        if has_negative:\n",
        "            for token in tokens:\n",
        "                if token not in seed_positive and token not in seed_negative:\n",
        "                    negative_cooccur[token] += 1\n",
        "    \n",
        "    # Find words that strongly associate with one sentiment\n",
        "    discovered_positive = []\n",
        "    discovered_negative = []\n",
        "    \n",
        "    all_words = set(positive_cooccur.keys()) | set(negative_cooccur.keys())\n",
        "    \n",
        "    for word in all_words:\n",
        "        pos_count = positive_cooccur.get(word, 0)\n",
        "        neg_count = negative_cooccur.get(word, 0)\n",
        "        total = pos_count + neg_count\n",
        "        \n",
        "        if total < 5:  # Filter rare words\n",
        "            continue\n",
        "        \n",
        "        # Calculate sentiment ratio\n",
        "        if pos_count > neg_count * 2:  # Strongly positive\n",
        "            discovered_positive.append((word, pos_count, pos_count / total))\n",
        "        elif neg_count > pos_count * 2:  # Strongly negative\n",
        "            discovered_negative.append((word, neg_count, neg_count / total))\n",
        "    \n",
        "    # Sort by count\n",
        "    discovered_positive.sort(key=lambda x: x[1], reverse=True)\n",
        "    discovered_negative.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return discovered_positive[:top_n], discovered_negative[:top_n]\n",
        "\n",
        "# Discover new sentiment words\n",
        "print(\"Discovering new sentiment words from the corpus...\\n\")\n",
        "new_positive, new_negative = discover_sentiment_words(\n",
        "    df['cleaned_text'], \n",
        "    POSITIVE_SEEDS, \n",
        "    NEGATIVE_SEEDS,\n",
        "    top_n=30\n",
        ")\n",
        "\n",
        "print(\"Discovered Positive Words:\")\n",
        "print(\"Word\\t\\tCount\\tRatio\")\n",
        "print(\"-\" * 40)\n",
        "for word, count, ratio in new_positive[:15]:\n",
        "    print(f\"{word:15s}\\t{count}\\t{ratio:.2f}\")\n",
        "\n",
        "print(\"\\nDiscovered Negative Words:\")\n",
        "print(\"Word\\t\\tCount\\tRatio\")\n",
        "print(\"-\" * 40)\n",
        "for word, count, ratio in new_negative[:15]:\n",
        "    print(f\"{word:15s}\\t{count}\\t{ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expand_header"
      },
      "source": [
        "## 7. Expand Lexicon with Discovered Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "expand_lexicon"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expanded positive lexicon: 30 → 52\n",
            "Expanded negative lexicon: 31 → 47\n",
            "\n",
            "New positive words added:\n",
            "{'جائزه', 'الفني', 'الفنان', 'ملال', 'الفنيه', 'المهرجان', 'حسبان', 'الفيلم', 'الاغنيه', 'الفنانين', 'بفضل', 'فيلم', 'بني', 'الاعمال', 'الفنانه', 'السيد', 'الموسيقي', 'مجال', 'مهرجان', 'السينما', 'اغنيه', 'المخرج'}\n",
            "\n",
            "New negative words added:\n",
            "{'لجامعه', 'السله', 'الاتحادات', 'عرضيه', 'الفهري', 'العصب', 'جامعه', 'بلاتر', 'دومو', 'غايبي', 'المديري', 'حياتو', 'السنوسي', 'جامعي', 'لقجع', 'النسويه'}\n"
          ]
        }
      ],
      "source": [
        "# Expand lexicon with high-confidence discovered words\n",
        "EXPANDED_POSITIVE = POSITIVE_SEEDS.copy()\n",
        "EXPANDED_NEGATIVE = NEGATIVE_SEEDS.copy()\n",
        "\n",
        "# Add discovered words with ratio > 0.7\n",
        "for word, count, ratio in new_positive:\n",
        "    if ratio > 0.7 and count > 10:\n",
        "        EXPANDED_POSITIVE.add(word)\n",
        "\n",
        "for word, count, ratio in new_negative:\n",
        "    if ratio > 0.7 and count > 10:\n",
        "        EXPANDED_NEGATIVE.add(word)\n",
        "\n",
        "print(f\"Expanded positive lexicon: {len(POSITIVE_SEEDS)} → {len(EXPANDED_POSITIVE)}\")\n",
        "print(f\"Expanded negative lexicon: {len(NEGATIVE_SEEDS)} → {len(EXPANDED_NEGATIVE)}\")\n",
        "\n",
        "print(\"\\nNew positive words added:\")\n",
        "print(EXPANDED_POSITIVE - POSITIVE_SEEDS)\n",
        "\n",
        "print(\"\\nNew negative words added:\")\n",
        "print(EXPANDED_NEGATIVE - NEGATIVE_SEEDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reanalyze_header"
      },
      "source": [
        "## 8. Re-analyze with Expanded Lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "reanalyze"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Re-analyzing with expanded lexicon...\n",
            "✓ Re-analysis complete\n",
            "\n",
            "Sentiment Distribution (Expanded Lexicon):\n",
            "final_sentiment\n",
            "Neutral              60350\n",
            "Positive             20334\n",
            "Somewhat Positive    14225\n",
            "Somewhat Negative     7383\n",
            "Negative              6495\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Comparison: Original vs Expanded\n",
            "final_sentiment    Negative  Neutral  Positive  Somewhat Negative  \\\n",
            "lexicon_sentiment                                                   \n",
            "Negative               2807       74        64                183   \n",
            "Neutral                1586    59201     10470               2149   \n",
            "Positive                 20       25      4933                 22   \n",
            "Somewhat Negative      1971      763       415               4902   \n",
            "Somewhat Positive       111      287      4452                127   \n",
            "\n",
            "final_sentiment    Somewhat Positive  \n",
            "lexicon_sentiment                     \n",
            "Negative                          32  \n",
            "Neutral                         8169  \n",
            "Positive                          81  \n",
            "Somewhat Negative                269  \n",
            "Somewhat Positive               5674  \n"
          ]
        }
      ],
      "source": [
        "# Re-analyze with expanded lexicon\n",
        "print(\"Re-analyzing with expanded lexicon...\")\n",
        "\n",
        "results_v2 = []\n",
        "for text in df['cleaned_text']:\n",
        "    result = analyze_text_sentiment_lexicon(\n",
        "        text, EXPANDED_POSITIVE, EXPANDED_NEGATIVE,\n",
        "        INTENSIFIERS, NEGATIONS\n",
        "    )\n",
        "    results_v2.append(result)\n",
        "\n",
        "# Add new results\n",
        "df['final_sentiment'] = [r['sentiment'] for r in results_v2]\n",
        "df['final_score'] = [r['score'] for r in results_v2]\n",
        "df['final_confidence'] = [r['confidence'] for r in results_v2]\n",
        "\n",
        "print(\"✓ Re-analysis complete\\n\")\n",
        "\n",
        "print(\"Sentiment Distribution (Expanded Lexicon):\")\n",
        "print(df['final_sentiment'].value_counts())\n",
        "\n",
        "print(\"\\nComparison: Original vs Expanded\")\n",
        "comparison = pd.crosstab(df['lexicon_sentiment'], df['final_sentiment'])\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clustering_header"
      },
      "source": [
        "## 9. Clustering-Based Sentiment Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "clustering"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing clustering analysis...\n",
            "✓ Clustering complete\n",
            "\n",
            "Cluster distribution:\n",
            "cluster\n",
            "0    13639\n",
            "1    17405\n",
            "2    10183\n",
            "3    46721\n",
            "4    20839\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Cluster Analysis:\n",
            "\n",
            "Cluster 0 (n=13639):\n",
            "  Most common sentiment: Neutral\n",
            "  Top words: علي, الي, ان, القضائيه, العامه, عناصر, المتهم, انه, الشرطه, النيابه\n",
            "\n",
            "Cluster 1 (n=17405):\n",
            "  Most common sentiment: Neutral\n",
            "  Top words: ان, علي, الي, المنتخب, الوطني, القدم, كاس, لكره, المباراه, مباراه\n",
            "\n",
            "Cluster 2 (n=10183):\n",
            "  Most common sentiment: Neutral\n",
            "  Top words: ان, علي, الي, الحكومه, حزب, الحزب, بنكيران, رئيس, والتنميه, العداله\n",
            "\n",
            "Cluster 3 (n=46721):\n",
            "  Most common sentiment: Neutral\n",
            "  Top words: ان, علي, الي, المغرب, خلال, المغربيه, انه, او, المائه, المغربي\n",
            "\n",
            "Cluster 4 (n=20839):\n",
            "  Most common sentiment: Neutral\n",
            "  Top words: ان, علي, الي, الفريق, الرياضي, المباراه, الرجاء, الموسم, مباراه, امام\n"
          ]
        }
      ],
      "source": [
        "# Use TF-IDF + K-Means to discover sentiment clusters\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "print(\"Performing clustering analysis...\")\n",
        "\n",
        "# Create TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
        "X = vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Reduce dimensionality for visualization\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "X_reduced = svd.fit_transform(X)\n",
        "\n",
        "# Cluster into 5 groups (matching sentiment categories)\n",
        "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "df['cluster'] = clusters\n",
        "\n",
        "print(f\"✓ Clustering complete\\n\")\n",
        "print(\"Cluster distribution:\")\n",
        "print(df['cluster'].value_counts().sort_index())\n",
        "\n",
        "# Analyze each cluster\n",
        "print(\"\\nCluster Analysis:\")\n",
        "for cluster_id in range(5):\n",
        "    cluster_texts = df[df['cluster'] == cluster_id]['cleaned_text']\n",
        "    cluster_sentiments = df[df['cluster'] == cluster_id]['final_sentiment']\n",
        "    \n",
        "    # Get most common words in cluster\n",
        "    all_words = ' '.join(cluster_texts).split()\n",
        "    word_freq = Counter(all_words).most_common(10)\n",
        "    \n",
        "    print(f\"\\nCluster {cluster_id} (n={len(cluster_texts)}):\")\n",
        "    print(f\"  Most common sentiment: {cluster_sentiments.mode()[0] if len(cluster_sentiments) > 0 else 'N/A'}\")\n",
        "    print(f\"  Top words: {', '.join([w for w, c in word_freq])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_header"
      },
      "source": [
        "## 10. Comparison with Original Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "comparison"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison: Our Lexicon-Based Analysis vs Original Labels\n",
            "\n",
            "Agreement rate: 15.80%\n",
            "\n",
            "Cross-tabulation:\n",
            "original_label     Negative  Neutral  Positive  Somewhat Negative  \\\n",
            "final_sentiment                                                     \n",
            "Negative                 77      100      5873                102   \n",
            "Neutral                2539     8504     22597              13312   \n",
            "Positive               9050     2766      4982               1010   \n",
            "Somewhat Negative       226      397      4935                619   \n",
            "Somewhat Positive      1844     2398      5288               1685   \n",
            "All                   13736    14165     43675              16728   \n",
            "\n",
            "original_label     Somewhat Positive     All  \n",
            "final_sentiment                               \n",
            "Negative                         343    6495  \n",
            "Neutral                        13398   60350  \n",
            "Positive                        2526   20334  \n",
            "Somewhat Negative               1206    7383  \n",
            "Somewhat Positive               3010   14225  \n",
            "All                            20483  108787  \n",
            "\n",
            "Examples where our analysis differs from original labels:\n",
            "\n",
            "Text: بين أستوديوهات ورزازات وصحراء مرزوكة وآثار وليلي ثم الرباط والبيضاء انتهى المخرج المغربي سهيل بن بركة من تصوير مشاهد عمله السينمائي الجديد الذي خصصه لتسليط الضوء عن حياة الجاسوس الإسباني دومينغو باديا الذي عاش فترة من القرن التاسع عشر بالمغرب باسم علي باي هذا الفيلم الذي اختار له مخرجه عنوان حلم خليفة يصور حياة علي باي العباسي الذي ما زال أحد أحياء طنجة يحمل اسمه عاش حياة فريدة متنكرا بشخصية تاجر عربي من سلالة الرسول صلى الله عليه وسلم فيما كان يعمل جاسوسا لحساب إسبانيا وكشف مخرج الفيلم سهيل بن بركة في تصريح لهسبريس أن الفيلم السينمائي دخل مرحلة التوضيب التي تتم خارج المغرب مبرزا أن الفيلم الذي يروي حياة الجاسوس الإسباني دومينغو باديا منذ أن قرر من طنجة بدء رحلاته نحو عدد من المناطق في العالم الإسلامي بداية القرن العشرين سيكون جاهزا بعد شهرين ويجمع الفيلم السينمائي عددا من الممثلين من مختلف الجنسيات واختار لدور البطولة الممثلة السينمائية الإيطالية كارولينا كريشنتيني للقيام بدور الإنجليزية الليدي هستر ستانهوب التي اشتهرت في الكتب الغربية بـ زنوبيا والتي عاشت بدورها بالدول العربية وارتبطت بعلي باي بعلاقة عاطفية إضافة إلى وجوه سينمائية معروفة وعن اختيار المخرج المغربي لحياة علي باي العباسي يوضح في تصريح لوكالة الأنباء الفرنسية هذه الشخصية عاشت أحداثا مشوقة كثيرة تستحق أن تسلط عليها الأضواء مشيرا إلى أن الفيلم سيحمل الكثير من المفاجآت لا سيما أن البطل قتل على يد امرأة دست له السم خلال رحلة الحج وأضاف شخصية طموحة وشجاعة ومثقفة ومذهلة في آن واحد كان يرى نفسه مستكشفا في أول الأمر نال علي باي إعجاب السلطان بعلمه فجعله من المقربين منه في ظرف وجيز ودعاه إلى اللحاق به إلى فاس وبرحيله إلى فاس تنتهي قصته مع طنجة وعاش علي باي العباسي بمدينة طنجة على أنه رجل مسلم أصله من الشام ونال ثقة الجميع في هذه المدينة حيث تم تشييد تمثال له في عروسة الشمال نظرا لتمكنه من بعض العلوم خاصة علم الفلك الذي مكنه من رصد كسوف الشمس الذي تزامن مع وجوده في طنجة فكان لعلمه دور كبير ساعده في إخفاء هويته كما أبان هذا الأمر أيضا عن تراجع كبير في ميدان العلم والمعرفة لدى المغاربة والمسلمين بصفة عامة\n",
            "  Original label: Negative\n",
            "  Our analysis: Positive (score: +8)\n",
            "\n",
            "Text: قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر عملها على الفن بل عملت مع أحد المتخصصين لإطلاق نوع جديد من الشاي سيصبح متوفرا ابتداء من الشهر المقبل في سلسلة مقاهي ستاربكس ونقلت وسائل إعلام أمريكية عن رئيس مجلس إدارة ستاربكس هاورد شولتز ووينفري إعلانهما عن ابتكار نوع جديد من الشاي يحمل اسم الذي سيباع ابتداء من أبريل نيسان المقبل في مقاهي ستاربكس وتيفانا بأمريكا وكندا وتعتزم ستاربكس التبرع بعائدات بيع هذا الشاي لأكاديمية أسستها وينفري وتعنى بتوفير فرص تعليم للشبان\n",
            "  Original label: Negative\n",
            "  Our analysis: Neutral (score: +0)\n",
            "\n",
            "Text: أخبارنا المغربية الوزاني تصوير الشملالي ألهب النجم المغربي الدوزي حماس أزيد من ألف متفرج أثثوا فضاءات مسرح الهواء الطلق بمدينة المضيق ليلة أمس الخميس يوليوز في إطار فعاليات مهرجان جولة الذي دأبت على تنظميه شركة اتصالات المغرب صيف كل سنة اعتلى الفنان الدوزي منصة المضيق في حدود منتصف الليل عرفت فقرات سهرته تجاوبا رائعا من طرف الجمهور الشمالي أيضا الوافد من مختلف المدن المغربية مع أغانيه القديمة والجديدة منها أغنية مريما فهميني حبيب الروح كما أدى بعض أغاني الركادة الشعبي بمناسبة تصادف الحفل مع الذكرى الـ لعيد العرش المجيد جعل الدوزي مسك ختام سهرته أغنية العيون عينية التي تفاعل الجمهور معها بشكل كبير جدا ليصنع مشهدا جميلا أثبت أن الفن قادر على تحريك المشاعر الوطنية الاعتزاز بالوطن عرفت سهرة الامس حضور نائب المدير التنفيذي لشركة اتصالات المغرب بعض رجال السلطات المحلية شهدت تنظيما محكما من طرف الشركة المنظمة للمهرجان الذي سيشهد في الايام القليلة المقبلة حضور أسماء فنية عربية مغربية نذكر منها وائل جسار فارس كرم سعيدة شرف سعيد موسكير\n",
            "  Original label: Negative\n",
            "  Our analysis: Positive (score: +5)\n",
            "  Positive words: سعيد\n",
            "\n",
            "Text: اخبارنا المغربية قال ابراهيم الراشدي محامي سعد المجرد ان موكله ذهب ضحية كمين نصب له من طرف شخصين هما فرنسي من اصل جزائري الثاني مصري مبرزا ان التحقيق في القضية سيوضح الحقيقة حول هوية الفتاة المشتكية قال الراشدي في تصريحات اذاعية انها من جنسية فرنسية لا تنحدر من الجزائر او بلد اخر مضيفا انها تبلغ من العمر سنة تعمل في علبة ليلية اوضح المتحدث ان الفتاة نفت في شكايتها لدى الشرطة وقوع علاقة جنسية مع سعد قدمت شهادة طبية مدتها يومين فقط مضيفا انها لا تعاني من اي رضوض في جسمها كما انها ذهبت برضاها مع سعد لم تثبت كاميرات الفندق وجود اي احتجاز او اكراه حول سبب عدم اطلاق سراح سعد المجرد متابعته في حالة سراح اكد الراشدي ان القضاء الفرنسي رفض الامر قبل وقوع المواجهة مع المشتكية تفاديا للتاثير عليها لان خروجه من السجن متابعته في حالة سراح قد يؤثر على الفتاة المشتكية نظرا لوضعيته الاجتماعية المتميزة اكد المحامي ان البث في طلب الاستئناف من اجل اطلاق سراح المجرد تقرر البث فيه في ظرف ايام سيتم النظر فيه يوم الاربعاء المقبل على اعتبار ان الثلاثاء يوم عطلة بفرنسا نفى الراشدي ما تداولته الصحافة الفرنسية حول اتهامه للجزائر بالوقوف وراء المؤامرة التي تعرض لها موكله مبرزا انه رجل قانون لا يخوض في امور لها علاقة بالسياسة\n",
            "  Original label: Negative\n",
            "  Our analysis: Neutral (score: +0)\n",
            "\n",
            "Text: تزال صناعة الجلود في المغرب تتبع الطريقة التقليدية واليدوية وتستخدم مواد طبيعية مائة بالمائة تنتشر معظم المدابغ في فاس التي اشتهرت بهذه المهنة منذ قرون ولننا اليوم اخترنا تسليط الضوء على إحدى المدابغ في مراكش والتي يتوافد عليها السياح رغم الرائحة الكريهة يعمل العمال تحت أشعة الشمس على معالجة الجلود عبر غمسها في المياه مع إضافة الليمون وفضلات الطيور والكلاب ويستخدمون لتلوينها أصباغا طبيعية تبدأ معالجة الجلود التي تصل إلى المدبغة في عربات تجرها الحمير عبر فرزها ثم توضع في الجفنة أو القصرية على شكل إناء كبير حسب النوع واللون ليتم نقعها في محلول من الجير والماء حتى يصبح الجلد ناعما ً، ويضاف إلى المحلول فضلات الكلاب أو الدجاج أو الحمام وهذا الخليط يمنح المكان رائحة نتنة تعود عليها العمال يتم نقع الجلد في المحلول لعدة أيام وأسابيع وإذا لزم الأمر يعاد نقعها عدة مرات بعد النقع تأتي عملية التنظيف إذ يتم إخراج الجلود من القصرية لتنظيفها يدويا من الصوف العلق وبعدها يتم غسل الجلد في ماء نضيف ويترك تحت أشعة الشمس ليجف الخطوة التالية هي صبغ الجلد إذ يتم وضعه مرة أخرى في قصرية من الماء الملون بصبغة طبيعية ويترك لعدة أسابيع ويتم تحريك الجلد عبر جميع الاتجاهات باستمرار حتى تتوزع الصبغة على جميع أنحائه هذه الصبغات هي مواد طبيعية مصنوعة من خليط الزعفران الأصفر والأحمر والفلفل في المرحلة الأخيرة تذهب الجلود المصنعة إلى ورش العمل حيث تتم صناعة الحقائب والأحذية التقليدية والرياضية وغير ذلك يتوافد على مدابغ الجلود عدد كبير من السياح لاسيما الأجانب رغم الرائحة الكريهة ويحرصون على شراء النعناع قبل الوصول إليها لغرض تخفيف شدة الرائحة\n",
            "  Original label: Negative\n",
            "  Our analysis: Neutral (score: +0)\n",
            "\n",
            "Text: مصطفى الصوفي انطلقت أمس فعاليات الدورة الثالثة من مهرجان سلا الدولي للفنون الساخرة التي تنظمها جمعية نجمة للفكر والإبداع والتي تستمر على مدى ثلاثة أيام تحت شعار نضحكوها مغربية وشهد حفل الافتتاح الذي عرف تقديم كلمة مهمة لرئيسة الجمعية فوزية الوافي تكريم الثنائي سفاج ومهيول هذا الثنائي الذي اعتبرته مديرة المهرجان زهور الزرييق من الثنائيات الكوميدية الناجحة والرائعة التي قدمت طيلة مسيرتها الفنية فيضا من الأعمال التي خلقت الفرجة الكوميدية للجمهور وبالرغم من مرض واحد من الثنائي والمقبل على عملية جراحية حيث نتمنى له الشفاء العاجل لا أن الثنائي المحبوب حل إلى المركب الثقافي الملكي من اجل لقاء الجمهور لقاء تميز بالبساطة والود وشهادة مؤثرة ومميزة قدمها الفنان الساخر الشرقي الساروتي في حق المحتفى بهما واللذين تلقيا هدايا وتذكارات من يد رئيسة الجمعية ورئيسة الجماعة الحضرية كما تميزت الدورة أيضا بتكريم الفنان عبد الخالق فيهد فضلا عن سلسة من الفقرات الكوميدية التي سيؤديها نجوم الكوميديا من الجيل الصاعد الفائزين في المباراة الخاصة بانتقاء أحسن الكوميديين الشباب من بروع الوطن ومن مدينة سلا وتشكل هذه الدورة بالنظر إلى برمجتها حلقة استثنائية بطاقمها التنظيمي النسائي حرصا من مديرة المهرجان زهور الزرييق على تحقيق مقاربة النوع وإعطاء الفرصة للمرأة من أجل ابرزا قدراتها ودعمها ومشاركتها الفعالة في هذه الدورة التي تقام بشراكة مع مؤسسة سلا للثقافة والفنون وبدعم من وزارة الثقافة والمسرح الوطني وبريد المغرب وشركة اوبامي حيث يسعى الجميع إلى تبيان قدرات الشباب ومنحهم ثقة كبيرة في مواهبهم وفرصة مواتية للمشاركة إلى جانب نجوم الكوميديا كما تتنوع فقرات الدورة بين عروض فنية ساخرة فردية وثنائية وفن عروض الحلقة بساحة باب الخميس فضلا عن تنظيم ندوة فكرية في إطار صالون الزهراء لفنون القول نظمت أمس حول موضوع مظاهر الكتابة الساخرة في التراث الفني يديرها الدكتور عبد المجيد فنيش والدكتور محمد رمصيص مع قراءة في ديوان طرامواي للزجال مراد القادري وهي الندوة التي أبرزت دور الكوميديا والسخرية في معالجة القضايا الاجتماعية ومحاربة الاستعمار الفرنسي هذا فضلا عن فقرات عدة من أبرزها عرض مسرحية واش الغالية رخيصة من إخراج وتأليف الفنان المتميزة زهور الزرييق وبطولة فاطمة الزهراء أحرار وبنعبد الله الجندي وغيرهما في حفل الاختتام وهي المسرحية التي تعالج قضايا المرأة وقضايا اجتماعية عدة في قالب فني ساخر وجميل\n",
            "  Original label: Negative\n",
            "  Our analysis: Positive (score: +10)\n",
            "  Positive words: احسن\n",
            "\n",
            "Text: الاتحاد الاشتراكي تحيي الفنانة اللبنانية نجوى كرم وضيفة شرف مهرجان تيميتار بأكادير حفل الاختتام الذي سينظم يوم الخامس والعشرين من يونيو الجار وسينشط هذا الحفل إلى جانب نجوى كرم الفنانة الشعبية المغربية زينة الداودية يذكر أن الفنانة اللبنانية قررت طرح ألبومها الجديد يوم من الشهر الجاري بعد مجموعة كبيرة من التأجيلات ويتضمن الألبوم ثماني أغاني لبنانية متنوعة منها عيني بعينك لو بس تعرف دلل بياع اليانصيب التي ربما سيكون لجمهور تيميتار نصيب منها\n",
            "  Original label: Negative\n",
            "  Our analysis: Positive (score: +4)\n",
            "\n",
            "Text: أقدمت كاثريونا وايت حبيبة الممثل جيم كاري على الانتحار في حادثة أعرب الكوميدي البالغ عاما عن عميق حزنه وصدمته حيالها وأشار المتحدث باسم شرطة لوس أنجليس فريد كورال إلى عثور هيئات الطوارئ الطبية على جثة امرأة في الثلاثين من عمرها متحدرة من شيرمان اوكس وقد حدد سبب الوفاة على أنه انتحار على الأرجح وتم فتح تحقيق بالحادثة ولفتت الصحافة المتخصصة في أخبار المشاهير الأمريكيين إلى أن الثنائي انفصلا الخميس الماضي بعد علاقة عاطفية صعبة انفصلا خلالها مرات عدة في السابق وذكر موقع تي ام زي نقلا عن مصادر في الشرطة أن وايت قضت جراء جرعة زائدة من الأدوية غير أن كورال لم يؤكد هذه المعلومات على الفور وأبدى كاري نجم أفلام عدة بينها دام اند دامر ايترنل سانشاين اوف ذي سبوتليس مايند مان اون ذي مون صدمته وعميق حزنه إزاء وفاة من وصفها بـ حبيبتي اللطيفة كاثريونا وقال في بيان حصلت وكالة فرانس برس على نسخة منه كانت زهرة إيرلندية حساسة ولطيفة حقا ً، كانت حساسيتها أكبر من أن تتحملها هذه الأرض وأضاف قلبي مع عائلتها وأصدقائها وجميع الذين أحبوها لقد حل علينا هذا الخبر جميعا كوقع الصاعقة\n",
            "  Original label: Negative\n",
            "  Our analysis: Neutral (score: +0)\n",
            "\n",
            "Text: أحمد الريدي تطور جديد شهدته قضية الفنانة زينة وأحمد عز والتوأم الحائر فيما بينهما وذلك بعدما قررت زينة التعليق على هذه المسألة للمرة الأولى من خلال مداخلة هاتفية أجرتها مع أحد البرامج التلفزيونية زينة أكدت في البداية أنها لن تتحدث عن القضية من الناحية القانونية لأنها تحترم قرار المحكمة بحظر النشر ولكنها عبرت عن غضبها بسبب المعلومات المغلوطة التي نشر ووجهت زينة رسالة إلى المشاهدين بألا يصدقوا ما يكتب لأنه غير صحيح مشيرة إلى أنها هي وأحمد عز هما من يمتلكان الحقيقة الكاملة بعدها وجهت زينة حديثها لأحمد عز قائلة يا أحمد يا عز عيب إللي بتعمله ده فضحت نفسك وفضحتنا معاك دي لا رجولة ولا أخلاق بتحارب ولادك الاتنين وهما طفلان ما يعرفوش حاجة وعيب لما تقف قدام طفلين وواحدة ست إللي هي أمهم وأنت إتجوزتها حديث زينة للمرة الأولى عن الأزمة جاء بعدما عقدت هيئة الدفاع عن أحمد عز مؤتمرا صحافيا مساء الثلاثاء الماضي من أجل الحديث عن الأزمة وتفنيدها بعدما أصدرت نيابة الأسرة تقريرا مبدئيا يعفي عز من نسب التوأم وتابعت زينة رسالتها إلى أحمد عز بنبرة حادة قائلة لو طلعت قولت للناس الحقيقة وإللي عندي وحقيقة كذبك عليهم إنت مش هتعرف تمشي في الشارع مؤكدة أن الجمهور لن يرحمه لأن الجمهور أصبح قاضيا عليهما ويحاكمهما على كل ما يقولانه وطلبت زينة من عز أن يكف عن إنفاق الأموال على المحامين وغيرهم وأن يصرف هذه الأموال على طفليه خاصة أنهما كانا مريضين خلال الفترة الماضية وظلت لمدة أربعة أشهر تتواجد إلى جوارهم في المستشفى كما أشارت إلى أنها لن تترك حقها قائلة أنا مش هسيب حقي يا أحمد يا عز ومش هسيب حق ولادي هاخد حقي من حبابي عنيك واعتبرت أن الأزمة تتمثل في كونها لا تحارب عدوا ً، ولكنها تحارب والد طفليها قائلة له إنت هتروح من ربنا فين إنت كدبت كدبة وصدقتها ووجهت زينة النصيحة لعز بأنه بدلا من أن يذهب إلى الخبراء في المحكمة ويلتقط الصور مع الموظفات فليقم بعمل تحليل الـ الذي أجرته هي بدورها أكثر من مرة كما اعتبرت أن الفنان من المفترض أن يكون لديه الإحساس بشكل أكبر ولكنها تساءلت عن مدى الإحساس لدى أحمد عز وطالبت زينة أن يتم إحضار شهادة التتبع الخاصة بعز ولماذا كان يتواجد بالولايات المتحدة الأميركية أثناء فترة حملها وقبل ولادتها مؤكدة أن لديها الكثير لتقوله لكنها لم تتحدث بعد لأنها تحترم أولادها والجمهور وأخيرا ً، زينة أكدت لعز أن الجمهور لم يعد يحبه الآن بسبب ما يقوم به تجاه طفلين مشيرة إلى كونها ستظل تحارب من أجل حق طفليها ووجهت رسالة أخيرة له قائلة استر ده ربنا بيستر كما أعربت عن تفاؤلها وثقتها في القضاء\n",
            "  Original label: Negative\n",
            "  Our analysis: Positive (score: +2)\n",
            "\n",
            "Text: أخبارنا المغربية بشراكة بين مؤسسة البشير للتعليم المدرسي الخصوصي والأكاديمية الجهوية للتربية والتكوين جهة مراكش تانسيفت الحوز والكوليزيوم القصصي بالمغرب يتم تنظيم ملتقى دراسي وطني حول القصة القصيرة بالمغرب القارئ في القصة القصيرة بالمغرب تشارك فيه نخبة رائدة في القصة القصيرة بالمغرب وسيتميز الملتقى بجلسات علمية تتوزع بين الجانب الدراسي لكل من محمد عزيز المصباحي ومحمد اشويكة ومحمد أمنصور وموحى وهبي ومحمد زهير وبين القراءات لكل من محمد تينفو وعلي الوكيلي وهشام المغاري وعمر العلوي ناسنا ونور الدين الظهرجي وبهية المعتضد كما ستخصص الجلسة الثالثة للاحتفاء بثلاثة أعلام وطنية في القصة القصيرة ويتعلق الأمر بكل من مصطفى المسناوي وادريس الصغير ومحمد زهير حيث سيقدم احمد بوزفور وسعيد الريحاني وعبد اللطيف السخيري شهادات وإضاءات حول هذه الشخصيات القصصية وتكرس الجلسة الرابعة للإعلان عن الفائز في المسابقة الجهوية في القصة القصيرة الإبداع مدخل للتربية والتي عرفت مشاركات مكثفة من مختلف مدارس الجهة وسيعرف الملتقى كذلك تقديم كتاب الشخصية بين الأدب والفن رؤى متقاطعة وهو من منشورات مؤسسة البشير للتعليم المدرسي الخصوصي وذلك يوم السبت أبريل بمؤسسة البشير بمراكش ابتداء من الساعة الرابعة مساء\n",
            "  Original label: Negative\n",
            "  Our analysis: Neutral (score: +0)\n"
          ]
        }
      ],
      "source": [
        "# Map original labels to sentiment names\n",
        "sentiment_mapping = {\n",
        "    0: 'Negative',\n",
        "    1: 'Somewhat Negative',\n",
        "    2: 'Neutral',\n",
        "    3: 'Somewhat Positive',\n",
        "    4: 'Positive'\n",
        "}\n",
        "\n",
        "df['original_label'] = df['targe'].map(sentiment_mapping)\n",
        "\n",
        "# Compare\n",
        "print(\"Comparison: Our Lexicon-Based Analysis vs Original Labels\\n\")\n",
        "\n",
        "# Agreement percentage\n",
        "agreement = (df['final_sentiment'] == df['original_label']).mean()\n",
        "print(f\"Agreement rate: {agreement:.2%}\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nCross-tabulation:\")\n",
        "crosstab = pd.crosstab(\n",
        "    df['final_sentiment'], \n",
        "    df['original_label'],\n",
        "    margins=True\n",
        ")\n",
        "print(crosstab)\n",
        "\n",
        "# Show examples of disagreement\n",
        "print(\"\\nExamples where our analysis differs from original labels:\")\n",
        "disagreements = df[df['final_sentiment'] != df['original_label']].head(10)\n",
        "for idx, row in disagreements.iterrows():\n",
        "    print(f\"\\nText: {row['text']}\")\n",
        "    print(f\"  Original label: {row['original_label']}\")\n",
        "    print(f\"  Our analysis: {row['final_sentiment']} (score: {row['final_score']:+.0f})\")\n",
        "    if row['positive_matches']:\n",
        "        print(f\"  Positive words: {row['positive_matches']}\")\n",
        "    if row['negative_matches']:\n",
        "        print(f\"  Negative words: {row['negative_matches']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive_header"
      },
      "source": [
        "## 11. Interactive Sentiment Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "interactive"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Interactive Arabic Sentiment Analyzer (Lexicon-Based)\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67f9400c65b64e41bec83ffd869905a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Textarea(value='', description='النص:', layout=Layout(height='100px', width='90%'), placeholder…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def analyze_custom_text(text):\n",
        "    \"\"\"Analyze sentiment of custom Arabic text\"\"\"\n",
        "    cleaned = preprocess_arabic_text(text, remove_stopwords=False)\n",
        "    \n",
        "    result = analyze_text_sentiment_lexicon(\n",
        "        cleaned, EXPANDED_POSITIVE, EXPANDED_NEGATIVE,\n",
        "        INTENSIFIERS, NEGATIONS\n",
        "    )\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(f\"📝 Original Text: {text}\")\n",
        "    print(f\"🧹 Cleaned Text: {cleaned}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"🎯 Sentiment: {result['sentiment']}\")\n",
        "    print(f\"📊 Score: {result['score']:+d} (Positive: +{result['positive_score']}, Negative: -{result['negative_score']})\")\n",
        "    print(f\"🔍 Confidence: {result['confidence']:.0%}\")\n",
        "    \n",
        "    if result['positive_matches']:\n",
        "        print(f\"\\n✅ Positive indicators: {', '.join(result['positive_matches'])}\")\n",
        "    \n",
        "    if result['negative_matches']:\n",
        "        print(f\"❌ Negative indicators: {', '.join(result['negative_matches'])}\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "\n",
        "# Create widget\n",
        "text_input = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='أدخل النص العربي هنا...',\n",
        "    description='النص:',\n",
        "    layout=widgets.Layout(width='90%', height='100px')\n",
        ")\n",
        "\n",
        "analyze_btn = widgets.Button(\n",
        "    description='🔍 تحليل المشاعر (Analyze)',\n",
        "    button_style='success'\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_click(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if text_input.value.strip():\n",
        "            analyze_custom_text(text_input.value)\n",
        "        else:\n",
        "            print(\"⚠️  الرجاء إدخال نص (Please enter text)\")\n",
        "\n",
        "analyze_btn.on_click(on_click)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Interactive Arabic Sentiment Analyzer (Lexicon-Based)\")\n",
        "print(\"=\"*70)\n",
        "display(widgets.VBox([text_input, analyze_btn, output]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## 12. Save Results and Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "save_files"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved files:\n",
            "  - arabic_sentiment_lexicon.json (sentiment word dictionaries)\n",
            "  - arabic_sentiment_analyzed.csv (analyzed dataset)\n",
            "\n",
            "Lexicon Statistics:\n",
            "  Positive words: 52\n",
            "  Negative words: 47\n",
            "  Intensifiers: 7\n",
            "  Negations: 7\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Save expanded lexicons\n",
        "lexicons = {\n",
        "    'positive_words': list(EXPANDED_POSITIVE),\n",
        "    'negative_words': list(EXPANDED_NEGATIVE),\n",
        "    'intensifiers': list(INTENSIFIERS),\n",
        "    'negations': list(NEGATIONS)\n",
        "}\n",
        "\n",
        "with open('arabic_sentiment_lexicon.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(lexicons, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Save analyzed dataset\n",
        "df.to_csv('arabic_sentiment_analyzed.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"✓ Saved files:\")\n",
        "print(\"  - arabic_sentiment_lexicon.json (sentiment word dictionaries)\")\n",
        "print(\"  - arabic_sentiment_analyzed.csv (analyzed dataset)\")\n",
        "\n",
        "print(f\"\\nLexicon Statistics:\")\n",
        "print(f\"  Positive words: {len(EXPANDED_POSITIVE)}\")\n",
        "print(f\"  Negative words: {len(EXPANDED_NEGATIVE)}\")\n",
        "print(f\"  Intensifiers: {len(INTENSIFIERS)}\")\n",
        "print(f\"  Negations: {len(NEGATIONS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates **unsupervised sentiment analysis** that doesn't rely on potentially incorrect labels:\n",
        "\n",
        "### ✅ What We Did:\n",
        "\n",
        "1. **Lexicon-Based Analysis**: Used Arabic sentiment word dictionaries\n",
        "2. **Pattern Recognition**: Detected negations, intensifiers, and compound expressions\n",
        "3. **Lexicon Expansion**: Discovered new sentiment words from text patterns\n",
        "4. **Clustering**: Grouped similar texts to find natural sentiment patterns\n",
        "5. **Validation**: Compared results with original (unreliable) labels\n",
        "\n",
        "### 💡 Advantages:\n",
        "\n",
        "- **No training needed**: Works immediately without labeled data\n",
        "- **Interpretable**: Shows exactly which words drove the sentiment decision\n",
        "- **Customizable**: Easy to add domain-specific words\n",
        "- **Language-aware**: Handles Arabic-specific patterns (negation, intensifiers)\n",
        "\n",
        "### 🔄 Next Steps:\n",
        "\n",
        "- Review discovered sentiment words and refine the lexicon\n",
        "- Add domain-specific terms for your use case\n",
        "- Use cluster analysis to find mislabeled examples\n",
        "- Combine lexicon scores with ML models for hybrid approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script filetype.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script websockets.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script distro.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script httpx.exe is installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "%pip install -qU langchain-google-genai langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m load_dotenv()\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain and Gemini model environment prepared.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# Retrieve the API key from Colab secrets\n",
        "\n",
        "\n",
        "# Initialize the Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "print(\"LangChain and Gemini model environment prepared.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
